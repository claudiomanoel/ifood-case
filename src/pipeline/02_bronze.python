# Databricks notebook source
# MAGIC %md
# MAGIC ### Example Exploratory Notebook
# MAGIC
# MAGIC Use this notebook to explore the data generated by the pipeline in your preferred programming language.
# MAGIC
# MAGIC **Note**: This notebook is not executed as part of the pipeline.

# COMMAND ----------


from datetime import datetime
from dateutil.relativedelta import relativedelta

DATA_FILE_TYPES = ['fhv_tripdata', 'fhvhv_tripdata', 'green_tripdata',  'yellow_tripdata']

def get_list_bronze_tables():
    """
    Retrieve the list of bronze tables from the workspace information schema.   
    :return: A list of table names in the '02_bronze' schema.
    """
    list_table_names = spark.sql("SELECT t.table_name FROM workspace.information_schema.tables as t where t.table_schema = '02_bronze'").select("table_name").collect()

    table_names = [f"workspace.02_bronze.{table_name['table_name']}" for table_name in list_table_names]
    
    return table_names

def import_bronze(date_to_ingest, file_type: str, list_table_names: list):
    """
    Import files from the specified path and create tables in the workspace.
    
    :param date_to_ingest: The date for which the files are to be imported.
    :param file_type: The type of file to be imported (e.g., 'fhv_tripdata', 'fhvhv_tripdata', etc.).
    :param list_table_names: A list of existing table names in the '02_bronze' schema.
    :return: None
    """

    transient_table_name = f"workspace.01_transient.{file_type}_{date_to_ingest.year}_{date_to_ingest.month:02}".strip()
    bronze_table_name = f"workspace.02_bronze.{file_type}".strip()

    if bronze_table_name.replace("workspace.02_bronze.", "") in list_table_names:
        sql = f"INSERT INTO {bronze_table_name} SELECT * FROM {transient_table_name};"
    else:
        sql = f"CREATE TABLE {bronze_table_name} TBLPROPERTIES ('delta.feature.timestampNtz' = 'supported') AS SELECT * FROM {transient_table_name};"
        
    print(sql)
    spark.sql(sql)

    print(f"Finishing importing {bronze_table_name} for {date_to_ingest.year}-{date_to_ingest.month:02}.")

def run(start_date=None, end_date=None):
    """ Run the import process for the specified date range and file types.
    """
    spark.sql("USE CATALOG `workspace`;")
    spark.sql("USE SCHEMA `02_bronze`;")

    list_table_names = get_list_bronze_tables()

    for file_type in DATA_FILE_TYPES:
        date_to_ingest = start_date
        while date_to_ingest <= end_date:
            import_bronze(date_to_ingest, file_type, list_table_names)

            if file_type not in list_table_names:
              list_table_names.append(file_type)

            date_to_ingest = date_to_ingest + relativedelta(months=1)

start_date = datetime(year=2023, month=1, day=1)
end_date = datetime(year=2023, month=5, day=1)
run(start_date, end_date)