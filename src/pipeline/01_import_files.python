# Databricks notebook source
# MAGIC %md
# MAGIC ### Example Exploratory Notebook
# MAGIC
# MAGIC Use this notebook to explore the data generated by the pipeline in your preferred programming language.
# MAGIC
# MAGIC **Note**: This notebook is not executed as part of the pipeline.

# COMMAND ----------


from datetime import datetime
from dateutil.relativedelta import relativedelta

data_file_types = ['fhv_tripdata', 'fhvhv_tripdata', 'green_tripdata', 
              'yellow_tripdata']

volume_path = '/Volumes/workspace/01_transient/new_york_trips'

start_date = datetime(year=2023, month=1, day=1)
end_date = datetime(year=2023, month=3, day=1)

spark.sql("USE CATALOG `workspace`;")
spark.sql("USE SCHEMA `01_transient`;")

for file_types in data_file_types:
    date_to_ingest = start_date
    while date_to_ingest <= end_date:
        file_name = f"{volume_path}/{file_types}_{date_to_ingest.year}-{date_to_ingest.month:02}.parquet"
        table_name = f"workspace.01_transient.{file_types}_{date_to_ingest.year}_{date_to_ingest.month:02}".strip()

        # Enable timestampNtz feature for the table
        enable_feature_sql = f"ALTER TABLE {table_name} SET TBLPROPERTIES ('delta.feature.timestampNtz' = 'supported');"
        spark.sql(enable_feature_sql)

        sql = f"CREATE OR REPLACE TABLE {table_name} AS SELECT * FROM PARQUET.`{file_name}`;"

        print(sql)

        date_to_ingest = date_to_ingest + relativedelta(months=1)

        spark.sql(sql)

# COMMAND ----------



spark.sql("CREATE OR REPLACE TABLE workspace.01_transient.fhv_tripdata_2023_01 AS SELECT * FROM PARQUET.`/Volumes/workspace/01_transient/new_york_trips/fhv_tripdata_2023-01.parquet`;")